{
  "rubric_metadata": {
    "rubric_name": "Week 2: The Automaton Auditor Self-Evaluation",
    "grading_target": "Week 2 Auditor Repository & Architectural Report",
    "version": "3.0.0"
  },
  "dimensions": [
    {
      "id": "git_forensic_analysis",
      "name": "Git Forensic Analysis",
      "target_artifact": "github_repo",
      "forensic_instruction": "Run 'git log --oneline --reverse' on the cloned repository. Count the total number of commits. Check if the commit history tells a progression story: Environment Setup -> Tool Engineering -> Graph Orchestration. Extract all commit messages and timestamps. Flag if there is a single 'init' commit or a 'bulk upload' pattern with no iterative development.",
      "success_pattern": "More than 3 commits showing clear progression from setup to tool engineering to graph orchestration. Atomic, step-by-step history with meaningful commit messages.",
      "failure_pattern": "Single 'init' commit or bulk upload of all code at once. No iterative development visible. Timestamps clustered within minutes.",
      "judicial_logic": {
        "prosecutor": "If history is a single commit or all timestamps within 10 minutes, charge 'Bulk Upload Fraud'. Score max 2. Extract exact timestamps as evidence.",
        "defense": "If commits show genuine iteration even with imperfect messages, credit the engineering process. Multiple commits in a day still show effort.",
        "tech_lead": "Assess whether commit granularity reflects real development workflow. Look for environment, tools, graph orchestration as distinct phases."
      }
    },
    {
      "id": "state_management_rigor",
      "name": "State Management Rigor",
      "target_artifact": "github_repo",
      "forensic_instruction": "Scan for 'src/state.py' or equivalent state definitions in 'src/graph.py'. Use AST parsing (not regex) to find classes inheriting from 'BaseModel' (Pydantic) or 'TypedDict'. Verify that the state actively maintains a collection of 'Evidence' objects and a list of 'JudicialOpinion' objects. Check for the use of 'operator.add' and 'operator.ior' as state reducers in 'Annotated' type hints to prevent data overwriting during parallel execution. Capture the full code snippet of the core 'AgentState' definition.",
      "success_pattern": "'AgentState' uses TypedDict or BaseModel with Annotated reducers. 'Evidence' and 'JudicialOpinion' are Pydantic BaseModel classes with typed fields. Reducers like 'operator.add' (for lists) and 'operator.ior' (for dicts) are present.",
      "failure_pattern": "Plain Python dicts used for state. No Pydantic models. No reducers, meaning parallel agents will overwrite each other's data.",
      "judicial_logic": {
        "prosecutor": "If Pydantic is missing entirely, score max 2. If reducers are absent, charge 'Parallel Data Corruption Risk'. Score max 3.",
        "defense": "If TypedDict is used correctly even without full Pydantic on domain objects, credit the structural intent. Partial implementation with clear understanding deserves score 3.",
        "tech_lead": "Verify operator.add vs operator.ior are used on the correct field types (list vs dict). Wrong reducer on wrong type is a functional bug."
      }
    },
    {
      "id": "graph_orchestration",
      "name": "Graph Orchestration Architecture",
      "target_artifact": "github_repo",
      "forensic_instruction": "Scan for the 'StateGraph' builder instantiation in 'src/graph.py'. Use AST parsing to analyze 'builder.add_edge()' and 'builder.add_conditional_edges()' calls. Determine if the Detectives (RepoInvestigator, DocAnalyst, VisionInspector) branch out from a single node and run concurrently (fan-out). Verify there is a synchronization node ('EvidenceAggregator' or equivalent) that collects all evidence before the Judges are invoked (fan-in). Verify the Judges (Prosecutor, Defense, TechLead) also fan-out in parallel from the aggregation node and fan-in before the ChiefJustice. Check for conditional edges that handle 'Evidence Missing' or 'Node Failure' scenarios. Capture the specific Python block defining the graph's nodes and edges.",
      "success_pattern": "Two distinct parallel fan-out/fan-in patterns: one for Detectives, one for Judges. Conditional edges handle error states. Graph structure: START -> [Detectives in parallel] -> EvidenceAggregator -> [Judges in parallel] -> ChiefJustice -> END.",
      "failure_pattern": "Purely linear flow (RepoInvestigator -> DocAnalyst -> Judge -> End). No parallel branches. No synchronization node. No conditional edges for error handling.",
      "judicial_logic": {
        "prosecutor": "If the graph is purely linear (A->B->C), charge 'Orchestration Fraud'. Score 1. No exceptions.",
        "defense": "Support simpler graph designs if they implement robust State transitions and Pydantic validation at every node. Partial parallelism (detectives parallel, judges linear) scores 3.",
        "tech_lead": "Determine if the fan-in synchronization correctly aggregates lists of evidence before passing them to the judicial bench. Check for conditional edges on error paths."
      }
    },
    {
      "id": "safe_tool_engineering",
      "name": "Safe Tool Engineering",
      "target_artifact": "github_repo",
      "forensic_instruction": "Scan 'src/tools/' for the repository cloning logic. Verify that 'tempfile.TemporaryDirectory()' or equivalent sandboxing is used for git clone operations. Check for raw 'os.system()' calls -- these are a security violation. Verify that 'subprocess.run()' or equivalent is used with proper error handling (capturing stdout/stderr, checking return codes). Ensure the cloned repo path is never the live working directory. Check that git authentication errors are handled gracefully. Capture the specific Python function responsible for executing the repository clone.",
      "success_pattern": "All git operations run inside 'tempfile.TemporaryDirectory()'. 'subprocess.run()' used with error handling. No raw 'os.system()' calls. Authentication failures caught and reported.",
      "failure_pattern": "Raw 'os.system(\"git clone <url>\")' drops code into the live working directory. No error handling around shell commands. No input sanitization on the repo URL.",
      "judicial_logic": {
        "prosecutor": "If raw os.system is found anywhere in the cloning logic, charge 'Security Negligence'. This overrides all effort points for this criterion. Score max 2.",
        "defense": "If subprocess is used even without full sandboxing, credit the intent. Lack of tempfile is Technical Debt, not Security Negligence.",
        "tech_lead": "Assess timeout handling, return code checking, stderr capture. These are production-grade requirements."
      }
    },
    {
      "id": "structured_output_enforcement",
      "name": "Structured Output Enforcement",
      "target_artifact": "github_repo",
      "forensic_instruction": "Scan Judge nodes in 'src/nodes/judges.py'. Verify that LLMs are invoked using '.with_structured_output()' or '.bind_tools()' bound to the Pydantic 'JudicialOpinion' schema. Check that the output includes 'score' (int), 'argument' (str), and 'cited_evidence' (list). Verify there is retry logic or error handling if a Judge returns freeform text instead of structured JSON. Capture the code block responsible for querying the Judge LLMs.",
      "success_pattern": "All Judge LLM calls use '.with_structured_output(JudicialOpinion)' or equivalent. Retry logic exists for malformed outputs. Output is validated against the Pydantic schema before being added to state.",
      "failure_pattern": "Judge nodes call LLMs with plain prompts and parse freeform text responses. No Pydantic validation on output. No retry on parse failure.",
      "judicial_logic": {
        "prosecutor": "If judges output free text, charge 'Hallucination Liability'. Score max 2 for Judicial Nuance criterion.",
        "defense": "If the intent to use structured output is present (even if partially implemented), award partial credit. Score 3 if JudicialOpinion schema is defined but binding is incomplete.",
        "tech_lead": "Check for retry loops with exponential backoff. Check that parse failures are caught and re-attempted, not silently dropped."
      }
    },
    {
      "id": "judicial_nuance",
      "name": "Judicial Nuance and Dialectics",
      "target_artifact": "github_repo",
      "forensic_instruction": "Scan 'src/nodes/judges.py' or prompt templates. Verify that Prosecutor, Defense, and Tech Lead personas have distinct, conflicting system prompts. Compare the three prompts -- if they share more than 50% of text, flag as 'Persona Collusion'. Check if the Prosecutor prompt includes adversarial language and instructions to look for gaps, security flaws, and laziness. Check if the Defense prompt includes instructions to reward effort, intent, and creative workarounds. Check if the Tech Lead prompt focuses on architectural soundness, maintainability, and practical viability. Verify the graph forces all three judges to run in parallel on the same evidence for each criterion.",
      "success_pattern": "Three clearly distinct personas with conflicting philosophies. Prompts actively instruct the model to be adversarial (Prosecutor), forgiving (Defense), or pragmatic (Tech Lead). Judges produce genuinely different scores and arguments for the same evidence.",
      "failure_pattern": "Single agent acts as 'The Grader' with no persona separation. Or three judges exist but share 90% of prompt text, producing near-identical outputs. Scores are random or purely praise/criticism without nuance.",
      "judicial_logic": {
        "prosecutor": "If the three judges share 90% of the same prompt text, charge 'Persona Collusion'. Score max 2. If outputs are free text, charge 'Hallucination Liability'.",
        "defense": "Look for specific prompt instructions that force the model to be contrarian or forgiving. Genuinely distinct personas with different philosophies earn full marks even if synthesis is simple.",
        "tech_lead": "Evaluate if the judges successfully map their opinions back to specific rubric criteria IDs. Verify retry logic exists for malformed judge outputs."
      }
    },
    {
      "id": "chief_justice_synthesis",
      "name": "Chief Justice Synthesis Engine",
      "target_artifact": "github_repo",
      "forensic_instruction": "Scan 'src/nodes/justice.py' for the ChiefJusticeNode implementation. Verify the conflict resolution uses hardcoded deterministic Python logic, not just an LLM prompt. Check for these specific rules: (1) Rule of Security -- if the Prosecutor identifies a confirmed security vulnerability, the score is capped at 3 regardless of Defense arguments. (2) Rule of Evidence -- if the Defense claims 'Deep Metacognition' but Detective evidence shows the artifact is missing, the Defense is overruled. (3) Rule of Functionality -- if the Tech Lead confirms the architecture is modular, this carries the highest weight for the Architecture criterion. Check if score variance > 2 triggers a specific re-evaluation rule. Verify the output is a structured Markdown report, not a console print.",
      "success_pattern": "Deterministic Python if/else logic implementing named rules (security override, fact supremacy, functionality weight). Score variance triggers specific re-evaluation. Output is a Markdown file with Executive Summary, Criterion Breakdown (with dissent), and Remediation Plan.",
      "failure_pattern": "ChiefJustice is just another LLM prompt that averages the three judge scores. No hardcoded rules. No dissent summary. Output is console text or unstructured.",
      "judicial_logic": {
        "prosecutor": "If ChiefJustice is a raw LLM call, charge 'Synthesis Negligence'. Score 1. The spec is explicit: hardcoded Python, not prompts.",
        "defense": "If the intent is clear and rules are partially implemented, award score 3. Partial determinism is better than pure LLM averaging.",
        "tech_lead": "Verify the three named rules are implemented as distinct code branches. Check that Markdown output follows Executive Summary -> Criterion Breakdown -> Remediation Plan."
      }
    },
    {
      "id": "theoretical_depth",
      "name": "Theoretical Depth (Documentation)",
      "target_artifact": "pdf_report",
      "forensic_instruction": "Search the PDF report for these specific terms: 'Dialectical Synthesis', 'Fan-In / Fan-Out', 'Metacognition', 'State Synchronization'. Determine if the term appears in a substantive architectural explanation or is just a buzzword dropped in the executive summary. Check if the report explains HOW the architecture executes these concepts, not just that they exist. Flag terms that appear without supporting explanation as 'Keyword Dropping'.",
      "success_pattern": "Terms appear in detailed architectural explanations. The report explains how Dialectical Synthesis is implemented via three parallel judge personas. Fan-In/Fan-Out is tied to specific graph edges. Metacognition is connected to the system evaluating its own evaluation quality.",
      "failure_pattern": "Terms appear only in the executive summary or introduction. No connection to actual implementation. 'We used Dialectical Synthesis' with no explanation of how.",
      "judicial_logic": {
        "prosecutor": "If key terms appear without architectural grounding, charge 'Keyword Dropping'. Each ungrounded term reduces the score by 1.",
        "defense": "If the report demonstrates genuine understanding even if terminology is slightly off, credit the substance over the labels.",
        "tech_lead": "Verify that Fan-In/Fan-Out explanations are tied to specific node names and edge definitions in the code, not abstract descriptions."
      }
    },
    {
      "id": "report_accuracy",
      "name": "Report Accuracy (Cross-Reference)",
      "target_artifact": "pdf_report",
      "forensic_instruction": "Extract all file paths mentioned in the PDF report (e.g., 'We isolated the AST logic in src/tools/ast_parser.py', 'We implemented parallel Judges in src/nodes/judges.py'). Cross-reference each claimed file path against the evidence collected by the RepoInvestigator. Build two lists: (1) Verified Paths -- files that the report mentions and actually exist in the repo. (2) Hallucinated Paths -- files the report claims exist but the RepoInvestigator found no evidence of. Flag any claims about features (e.g., 'We implemented parallel Judges') where the code evidence contradicts the claim.",
      "success_pattern": "All file paths mentioned in the report exist in the repo. Feature claims match code evidence. Zero hallucinated paths.",
      "failure_pattern": "Report references files that do not exist. Claims parallel execution but code shows linear flow. Multiple hallucinated paths detected.",
      "judicial_logic": {
        "prosecutor": "Each hallucinated path is a charge of 'Auditor Hallucination'. Three or more hallucinated paths = Score 1.",
        "defense": "If file paths are slightly different (e.g., tools.py vs repo_tools.py) but the content matches, credit the intent. Naming discrepancies are not hallucinations.",
        "tech_lead": "Verify that feature claims in the report (e.g., 'parallel judges') match the actual graph topology found by AST analysis."
      }
    },
    {
      "id": "swarm_visual",
      "name": "Architectural Diagram Analysis",
      "target_artifact": "pdf_images",
      "forensic_instruction": "Extract images from the PDF report. Classify each diagram: is it an accurate LangGraph State Machine diagram, a sequence diagram, or just generic flowchart boxes? Check if the diagram explicitly visualizes the parallel split: START -> [Detectives in parallel] -> Evidence Aggregation -> [Prosecutor || Defense || TechLead in parallel] -> Chief Justice Synthesis -> END. Verify the diagram distinguishes between parallel branches and sequential steps. Flag diagrams that show a simple linear pipeline as 'Misleading Architecture Visual'.",
      "success_pattern": "Diagram accurately represents the StateGraph with clear parallel branches for both Detectives and Judges. Fan-out and fan-in points are visually distinct. Flow matches the actual code architecture.",
      "failure_pattern": "Generic box-and-arrow diagram with no indication of parallelism. Or no diagram present at all. Diagram shows linear flow that contradicts the parallel architecture claimed in the report.",
      "judicial_logic": {
        "prosecutor": "If no diagram is present, score 1. If diagram shows linear flow when code is parallel, charge 'Misleading Architecture Visual'. Score 2.",
        "defense": "If diagram shows the correct high-level structure even without precise LangGraph notation, award partial credit (Score 3).",
        "tech_lead": "Verify that parallel branches are visually distinct from sequential steps. Fan-out and fan-in points must be identifiable."
      }
    }
  ],
  "synthesis_rules": {
    "security_override": "Confirmed security flaws (e.g., shell injection in git tools, raw os.system with unsanitized inputs) cap the total score at 3, overriding any effort points from the Defense.",
    "fact_supremacy": "Forensic evidence (facts from Detectives) always overrules Judicial opinion (interpretation from Judges). If the Defense claims 'Deep Metacognition' but the RepoInvestigator found no supporting code, the Defense is overruled for hallucination.",
    "functionality_weight": "If the Tech Lead confirms the architecture is modular and workable, this carries the highest weight for the 'Graph Orchestration Architecture' criterion.",
    "dissent_requirement": "The Chief Justice must summarize why the Prosecutor and Defense disagreed in the final report. Every criterion with a score variance > 2 must include an explicit dissent explanation.",
    "variance_re_evaluation": "If score variance across the three judges exceeds 2 for any criterion (e.g., Prosecutor says 1, Defense says 5), trigger a re-evaluation of the specific evidence cited by each judge before rendering the final score."
  }
}